"""
Code Generator untuk SimpleNeural-DSL
Menghasilkan kode Python (TensorFlow/Keras) dari AST
"""

from typing import List, Dict, Any
from .parser import (
    ProgramNode, DatasetNode, ModelNode, LayerNode,
    OptimizerNode, TrainConfigNode
)


class CodeGenerator:
    """Generator kode Python dari AST"""
    
    def __init__(self):
        self.code_lines: List[str] = []
        self.indent_level = 0
        
    def generate(self, ast: ProgramNode, output_file: str = "output.py") -> str:
        """Generate Python code from AST"""
        self.code_lines = []
        self.indent_level = 0
        
        # Generate code sections
        self.generate_header(output_file)
        self.generate_imports()
        
        if ast.dataset:
            self.generate_dataset_loader(ast.dataset)
        
        # Store model for later use
        model_obj = None
        for model in ast.models:
            self.generate_model(model)
            model_obj = model  # Keep reference to model
        
        self.generate_main(model_obj)
        
        # Join all lines
        return '\n'.join(self.code_lines)
    
    def emit(self, line: str = ""):
        """Emit a line of code with proper indentation"""
        if line:
            indent = "    " * self.indent_level
            self.code_lines.append(indent + line)
        else:
            self.code_lines.append("")
    
    def indent(self):
        """Increase indentation level"""
        self.indent_level += 1
    
    def dedent(self):
        """Decrease indentation level"""
        if self.indent_level > 0:
            self.indent_level -= 1
    
    def generate_header(self, source_file: str):
        """Generate file header"""
        self.emit('#!/usr/bin/env python3')
        self.emit('# -*- coding: utf-8 -*-')
        self.emit('"""')
        self.emit('Auto-generated by SimpleNeural-DSL Compiler')
        self.emit(f'Source: {source_file}')
        self.emit('"""')
        self.emit()
    
    def generate_imports(self):
        """Generate import statements"""
        self.emit('# ==================== IMPORTS ====================')
        self.emit('import tensorflow as tf')
        self.emit('import pandas as pd')
        self.emit('import numpy as np')
        self.emit('from sklearn.model_selection import train_test_split')
        self.emit('from sklearn.preprocessing import StandardScaler')
        self.emit('from sklearn.metrics import r2_score')
        self.emit('import warnings')
        self.emit('warnings.filterwarnings("ignore")')
        self.emit()
        self.emit('# Set random seeds for reproducibility')
        self.emit('tf.random.set_seed(42)')
        self.emit('np.random.seed(42)')
        self.emit()
        self.emit()
    
    def generate_dataset_loader(self, dataset: DatasetNode):
        """Generate dataset loading function"""
        self.emit('# ==================== DATA LOADING ====================')
        self.emit('def load_and_preprocess_data():')
        self.indent()
        self.emit('"""Load dataset and perform preprocessing."""')
        self.emit(f'print("[INFO] Loading dataset: {dataset.file_path}")')
        self.emit()
        
        # Load CSV
        self.emit('try:')
        self.indent()
        self.emit(f'df = pd.read_csv("{dataset.file_path}")')
        self.dedent()
        self.emit('except FileNotFoundError:')
        self.indent()
        self.emit('raise FileNotFoundError(')
        self.indent()
        self.emit(f'f"Dataset \'{dataset.file_path}\' not found. "')
        self.emit('"Please ensure the file is in the current directory."')
        self.dedent()
        self.emit(')')
        self.dedent()
        self.emit()
        
        # Drop ID columns
        self.emit('# Drop ID columns if present')
        self.emit("id_cols = [col for col in df.columns if col.lower() in ['id', 'index', 'unnamed: 0']]")
        self.emit('if id_cols:')
        self.indent()
        self.emit('print(f"[INFO] Dropping ID columns: {id_cols}")')
        self.emit('df = df.drop(id_cols, axis=1)')
        self.dedent()
        self.emit()
        
        # Separate features and target (case-insensitive)
        self.emit(f'# Separate features and target (case-insensitive)')
        self.emit(f'target_col = "{dataset.target_column}"')
        self.emit('target_col_actual = None')
        self.emit('for col in df.columns:')
        self.indent()
        self.emit('if col.lower() == target_col.lower():')
        self.indent()
        self.emit('target_col_actual = col')
        self.emit('break')
        self.dedent()
        self.dedent()
        self.emit()
        self.emit('if target_col_actual is None:')
        self.indent()
        self.emit('raise ValueError(')
        self.indent()
        self.emit(f'f"Target column {repr(dataset.target_column)} not found (case-insensitive). "')
        self.emit('f"Available columns: {list(df.columns)}"')
        self.dedent()
        self.emit(')')
        self.dedent()
        self.emit()
        
        self.emit('y = df.pop(target_col_actual)')
        self.emit('X = df')
        self.emit()
        
        # Check if classification task (categorical target)
        self.emit('# Detect if classification task')
        self.emit("is_classification = y.dtype == 'object' or y.nunique() < 20")
        self.emit('num_classes = 0')
        self.emit()
        self.emit('if is_classification:')
        self.indent()
        self.emit('print("[INFO] Classification task detected")')
        self.emit('from sklearn.preprocessing import LabelEncoder')
        self.emit('from tensorflow.keras.utils import to_categorical')
        self.emit('le = LabelEncoder()')
        self.emit('y_encoded = le.fit_transform(y)')
        self.emit('num_classes = len(le.classes_)')
        self.emit('print(f"[INFO] Classes ({num_classes}): {list(le.classes_)}")')
        self.emit('if num_classes > 2:')
        self.indent()
        self.emit('y = to_categorical(y_encoded, num_classes=num_classes)')
        self.emit('print("[INFO] Applied one-hot encoding for multi-class classification")')
        self.dedent()
        self.emit('else:')
        self.indent()
        self.emit('y = y_encoded  # Binary classification')
        self.dedent()
        self.dedent()
        self.emit('else:')
        self.indent()
        self.emit('print("[INFO] Regression task detected")')
        self.emit('num_classes = 0')
        self.dedent()
        self.emit()
        
        # Handle missing values
        self.emit('# Handle missing values')
        self.emit('if X.isnull().any().any():')
        self.indent()
        self.emit('print("[WARNING] Dataset contains missing values. Filling with mean...")')
        self.emit('X = X.fillna(X.mean())')
        self.dedent()
        self.emit()
        
        # Feature scaling
        self.emit('# Feature scaling')
        self.emit('print("[INFO] Applying StandardScaler to features...")')
        self.emit('scaler = StandardScaler()')
        self.emit('X_scaled = scaler.fit_transform(X)')
        self.emit()
        
        # Train-test split
        self.emit('# Train-test split')
        self.emit('X_train, X_test, y_train, y_test = train_test_split(')
        self.indent()
        self.emit('X_scaled, y,')
        self.emit('test_size=0.2,')
        self.emit('random_state=42')
        self.dedent()
        self.emit(')')
        self.emit()
        
        self.emit('print(f"[INFO] Training samples: {len(X_train)}")')
        self.emit('print(f"[INFO] Test samples: {len(X_test)}")')
        self.emit('print(f"[INFO] Number of features: {X_train.shape[1]}")')
        self.emit()
        
        self.emit('return X_train, X_test, y_train, y_test, scaler, is_classification, num_classes')
        self.dedent()
        self.emit()
        self.emit()
    
    def generate_model(self, model: ModelNode):
        """Generate model definition"""
        self.emit('# ==================== MODEL DEFINITION ====================')
        self.emit('def build_model(input_shape):')
        self.indent()
        self.emit(f'"""Build the neural network model: {model.name}"""')
        self.emit(f'print("[INFO] Building model: {model.name}")')
        self.emit()
        
        # Start Sequential model
        self.emit('model = tf.keras.Sequential([')
        self.indent()
        
        # Input layer
        self.emit('tf.keras.layers.InputLayer(input_shape=(input_shape,)),')
        
        # Generate layers
        for i, layer in enumerate(model.layers):
            layer_code = self.generate_layer(layer, i)
            self.emit(layer_code + ',')
        
        self.dedent()
        self.emit(f'], name="{model.name}")')
        self.emit()
        
        self.emit('return model')
        self.dedent()
        self.emit()
        self.emit()
        
        # Generate compile function
        self.emit('def compile_model(model, is_classification=False, num_classes=0):')
        self.indent()
        self.emit('"""Compile model with appropriate loss and metrics"""')
        if model.optimizer:
            optimizer_code = self.generate_optimizer(model.optimizer)
            self.emit('# Configure optimizer')
            self.emit(f'optimizer = {optimizer_code}')
        else:
            self.emit('# Default optimizer')
            self.emit('optimizer = tf.keras.optimizers.Adam()')
        self.emit()
        self.emit('if is_classification:')
        self.indent()
        self.emit('if num_classes > 2:')
        self.indent()
        self.emit('# Multi-class classification')
        self.emit('model.compile(')
        self.indent()
        self.emit('optimizer=optimizer,')
        self.emit('loss="categorical_crossentropy",')
        self.emit('metrics=["accuracy"]')
        self.dedent()
        self.emit(')')
        self.dedent()
        self.emit('else:')
        self.indent()
        self.emit('# Binary classification')
        self.emit('model.compile(')
        self.indent()
        self.emit('optimizer=optimizer,')
        self.emit('loss="binary_crossentropy",')
        self.emit('metrics=["accuracy"]')
        self.dedent()
        self.emit(')')
        self.dedent()
        self.dedent()
        self.emit('else:')
        self.indent()
        self.emit('# Regression')
        self.emit('model.compile(')
        self.indent()
        self.emit('optimizer=optimizer,')
        self.emit('loss="mse",')
        self.emit('metrics=["mae", "mse"]')
        self.dedent()
        self.emit(')')
        self.dedent()
        self.emit()
        self.emit('return model')
        self.dedent()
        self.emit()
        self.emit()
        
        # Generate training function
        if model.train_config:
            self.generate_training_function(model.train_config, model.name)
    
    def generate_layer(self, layer: LayerNode, index: int) -> str:
        """Generate code for a single layer"""
        layer_type = layer.layer_type
        params = layer.parameters
        
        if layer_type == 'DENSE':
            units = params.get('units', 64)
            activation = params.get('activation', 'relu')
            return f'tf.keras.layers.Dense({units}, activation="{activation}", name="dense_{index}")'
        
        elif layer_type == 'CONV2D':
            filters = params.get('filters', 32)
            kernel_size = params.get('kernel_size', '(3,3)')
            activation = params.get('activation', 'relu')
            return f'tf.keras.layers.Conv2D({filters}, {kernel_size}, activation="{activation}", name="conv2d_{index}")'
        
        elif layer_type == 'DROPOUT':
            rate = params.get('rate', 0.5)
            return f'tf.keras.layers.Dropout({rate}, name="dropout_{index}")'
        
        elif layer_type == 'FLATTEN':
            return f'tf.keras.layers.Flatten(name="flatten_{index}")'
        
        elif layer_type == 'LSTM':
            units = params.get('units', 64)
            return_sequences = params.get('return_sequences', False)
            return f'tf.keras.layers.LSTM({units}, return_sequences={return_sequences}, name="lstm_{index}")'
        
        elif layer_type == 'GRU':
            units = params.get('units', 64)
            return_sequences = params.get('return_sequences', False)
            return f'tf.keras.layers.GRU({units}, return_sequences={return_sequences}, name="gru_{index}")'
        
        elif layer_type == 'BATCHNORM':
            return f'tf.keras.layers.BatchNormalization(name="batchnorm_{index}")'
        
        elif layer_type == 'MAXPOOL2D':
            pool_size = params.get('pool_size', '(2,2)')
            return f'tf.keras.layers.MaxPooling2D({pool_size}, name="maxpool_{index}")'
        
        else:
            return f'# Unknown layer type: {layer_type}'
    
    def generate_optimizer(self, optimizer: OptimizerNode) -> str:
        """Generate optimizer code"""
        opt_type = optimizer.optimizer_type
        params = optimizer.parameters
        
        lr = params.get('lr', 0.001)
        
        if opt_type == 'adam':
            return f'tf.keras.optimizers.Adam(learning_rate={lr})'
        elif opt_type == 'sgd':
            momentum = params.get('momentum', 0.0)
            return f'tf.keras.optimizers.SGD(learning_rate={lr}, momentum={momentum})'
        elif opt_type == 'rmsprop':
            return f'tf.keras.optimizers.RMSprop(learning_rate={lr})'
        elif opt_type == 'adagrad':
            return f'tf.keras.optimizers.Adagrad(learning_rate={lr})'
        elif opt_type == 'adamw':
            return f'tf.keras.optimizers.AdamW(learning_rate={lr})'
        elif opt_type == 'nadam':
            return f'tf.keras.optimizers.Nadam(learning_rate={lr})'
        else:
            return f'tf.keras.optimizers.Adam(learning_rate={lr})'
    
    def generate_training_function(self, train_config: TrainConfigNode, model_name: str):
        """Generate training function"""
        params = train_config.parameters
        
        epochs = params.get('epochs', 10)
        batch_size = params.get('batch_size', 32)
        validation_split = params.get('validation_split', 0.2)
        
        self.emit('# ==================== TRAINING ====================')
        self.emit('def train_model(model, X_train, y_train, X_test, y_test):')
        self.indent()
        self.emit('"""Train the model with specified parameters."""')
        self.emit('print("\\n[INFO] Starting training...")')
        self.emit('print("=" * 50)')
        self.emit()
        
        # Callbacks
        self.emit('# Callbacks for better training control')
        self.emit('callbacks = [')
        self.indent()
        self.emit('tf.keras.callbacks.EarlyStopping(')
        self.indent()
        self.emit('monitor="val_loss",')
        self.emit('patience=20,  # Increased patience for better convergence')
        self.emit('restore_best_weights=True,')
        self.emit('verbose=1,')
        self.emit('min_delta=0.0001  # Stop if improvement < 0.0001')
        self.dedent()
        self.emit('),')
        self.emit('tf.keras.callbacks.ReduceLROnPlateau(')
        self.indent()
        self.emit('monitor="val_loss",')
        self.emit('factor=0.5,')
        self.emit('patience=8,  # Reduce LR after 8 epochs without improvement')
        self.emit('min_lr=1e-7,')
        self.emit('verbose=1')
        self.dedent()
        self.emit('),')
        self.emit('# Ensure models directory exists')
        self.emit('import os')
        self.emit('os.makedirs("models", exist_ok=True)')
        self.emit(f'model_checkpoint_path = f"models/{model_name.lower()}_best.keras"')
        self.emit('tf.keras.callbacks.ModelCheckpoint(')
        self.indent()
        self.emit('model_checkpoint_path,')
        self.emit('monitor="val_loss",')
        self.emit('save_best_only=True,')
        self.emit('verbose=0')
        self.dedent()
        self.emit(')')
        self.dedent()
        self.emit(']')
        self.emit()
        
        # Training
        self.emit('# Training')
        self.emit('history = model.fit(')
        self.indent()
        self.emit('X_train, y_train,')
        self.emit(f'epochs={epochs},')
        self.emit(f'batch_size={batch_size},')
        self.emit(f'validation_split={validation_split},')
        self.emit('callbacks=callbacks,')
        self.emit('verbose=1')
        self.dedent()
        self.emit(')')
        self.emit()
        
        self.emit('return history')
        self.dedent()
        self.emit()
        self.emit()
        
        # Evaluation function
        self.emit('# ==================== EVALUATION ====================')
        self.emit('def evaluate_model(model, X_test, y_test, is_classification=False):')
        self.indent()
        self.emit('"""Evaluate model performance on test set."""')
        self.emit('print("\\n[INFO] Evaluating model on test set...")')
        self.emit('print("=" * 50)')
        self.emit()
        
        self.emit('results = model.evaluate(X_test, y_test, verbose=0)')
        self.emit()
        
        self.emit('if is_classification:')
        self.indent()
        self.emit('print(f"Test Loss: {results[0]:.4f}")')
        self.emit('print(f"Test Accuracy: {results[1]:.4f}")')
        self.emit()
        self.emit('# Predictions')
        self.emit('predictions = model.predict(X_test, verbose=0)')
        self.emit('y_pred = np.argmax(predictions, axis=1) if len(predictions.shape) > 1 and predictions.shape[1] > 1 else (predictions > 0.5).astype(int)')
        self.emit('y_true = np.argmax(y_test, axis=1) if len(y_test.shape) > 1 and y_test.shape[1] > 1 else y_test')
        self.emit('from sklearn.metrics import classification_report')
        self.emit('print("\\nClassification Report:")')
        self.emit('print(classification_report(y_true, y_pred))')
        self.dedent()
        self.emit('else:')
        self.indent()
        self.emit('print(f"Test Loss (MSE): {results[0]:.4f}")')
        self.emit('print(f"Test MAE: {results[1]:.4f}")')
        self.emit('print(f"Test RMSE: {np.sqrt(results[0]):.4f}")')
        self.emit()
        self.emit('# Predictions')
        self.emit('predictions = model.predict(X_test, verbose=0)')
        self.emit()
        self.emit('# R² Score')
        self.emit('r2 = r2_score(y_test, predictions)')
        self.emit('print(f"R² Score: {r2:.4f}")')
        self.dedent()
        self.emit()
        
        self.emit('return results, predictions')
        self.dedent()
        self.emit()
        self.emit()
    
    def generate_main(self, model: ModelNode):
        """Generate main function"""
        self.emit('# ==================== MAIN ====================')
        self.emit('def main():')
        self.indent()
        self.emit('"""Main execution function."""')
        self.emit('print("=" * 60)')
        self.emit('print("SimpleNeural-DSL Generated Model")')
        self.emit('print("=" * 60)')
        self.emit()
        
        self.emit('# Load data')
        self.emit('X_train, X_test, y_train, y_test, scaler, is_classification, num_classes = load_and_preprocess_data()')
        self.emit()
        
        self.emit('# Build model')
        self.emit('model = build_model(input_shape=X_train.shape[1])')
        self.emit()
        
        self.emit('# Compile model with appropriate loss')
        self.emit('model = compile_model(model, is_classification, num_classes)')
        self.emit()
        
        self.emit('# Show model summary')
        self.emit('print("\\n[INFO] Model Architecture:")')
        self.emit('model.summary()')
        self.emit()
        
        self.emit('# Train model')
        self.emit('history = train_model(model, X_train, y_train, X_test, y_test)')
        self.emit()
        
        self.emit('# Evaluate model')
        self.emit('results, predictions = evaluate_model(model, X_test, y_test, is_classification)')
        self.emit()
        
        self.emit('# Save model')
        self.emit('import os')
        self.emit('os.makedirs("models", exist_ok=True)')
        self.emit(f'model_path = "models/{model.name.lower()}_final.keras"')
        self.emit('model.save(model_path)')
        self.emit('print(f"\\n[INFO] Model saved to: {model_path}")')
        self.emit(f'print("[INFO] Best model saved to: models/{model.name.lower()}_best.keras")')
        self.emit()
        
        self.emit('print("\\n" + "=" * 60)')
        self.emit('print("Training completed successfully!")')
        self.emit('print("=" * 60)')
        self.emit()
        
        self.emit('return model, history, scaler')
        self.dedent()
        self.emit()
        self.emit()
        
        self.emit('if __name__ == "__main__":')
        self.indent()
        self.emit('model, history, scaler = main()')
        self.dedent()


def main():
    """Test code generator"""
    from .lexer import Lexer
    from .parser import Parser
    
    test_code = '''
DATASET load "data.csv" TARGET "price"

MODEL "TestModel" {
    LAYER DENSE units: 64 activation: "relu"
    LAYER DROPOUT rate: 0.2
    LAYER DENSE units: 32 activation: "relu"
    LAYER DENSE units: 1 activation: "linear"
    
    OPTIMIZER "adam" lr: 0.01
    TRAIN epochs: 50 batch_size: 32 validation_split: 0.2
}
'''
    
    print("Source Code:")
    print("-" * 60)
    print(test_code)
    print()
    
    try:
        # Tokenize
        lexer = Lexer(test_code)
        tokens = lexer.tokenize()
        
        # Parse
        parser = Parser(tokens)
        ast = parser.parse()
        
        # Generate code
        print("Generating Python code...")
        print("=" * 60)
        
        generator = CodeGenerator()
        python_code = generator.generate(ast, "test.sndsl")
        
        print("\nGenerated Python Code:")
        print("=" * 60)
        print(python_code)
        
    except Exception as e:
        print(f"ERROR: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()
